Update 2005-07-22 gotom.
Enable asm-ppc64/byteorder.h patch again, moved from asm-ppc64-swab64.patch.
Remove linux/byteorder/swabb.h part, it's already applied to upstream.

Update 2005-05-27 gotom.
Disable asm-ppc64/byteorder.h patch, move it to asm-ppc64-swab64.patch.

Index: include/asm-alpha/byteorder.h
===================================================================
--- include/asm-alpha/byteorder.h.orig	2006-09-19 23:42:06.000000000 -0400
+++ include/asm-alpha/byteorder.h	2006-10-07 09:02:17.366543291 -0400
@@ -7,7 +7,7 @@
 
 #ifdef __GNUC__
 
-static __inline __attribute_const__ __u32 __arch__swab32(__u32 x)
+static __inline __u32 __arch__swab32(__u32 x)
 {
 	/*
 	 * Unfortunately, we can't use the 6 instruction sequence
Index: include/asm-m68k/byteorder.h
===================================================================
--- include/asm-m68k/byteorder.h.orig	2006-09-19 23:42:06.000000000 -0400
+++ include/asm-m68k/byteorder.h	2006-10-07 09:02:17.370543699 -0400
@@ -6,7 +6,7 @@
 
 #ifdef __GNUC__
 
-static __inline__ __attribute_const__ __u32 ___arch__swab32(__u32 val)
+static __inline__ __u32 ___arch__swab32(__u32 val)
 {
 	__asm__("rolw #8,%0; swap %0; rolw #8,%0" : "=d" (val) : "0" (val));
 	return val;
Index: include/asm-parisc/byteorder.h
===================================================================
--- include/asm-parisc/byteorder.h.orig	2006-09-19 23:42:06.000000000 -0400
+++ include/asm-parisc/byteorder.h	2006-10-07 09:02:17.370543699 -0400
@@ -6,7 +6,7 @@
 
 #ifdef __GNUC__
 
-static __inline__ __attribute_const__ __u16 ___arch__swab16(__u16 x)
+static __inline__ __u16 ___arch__swab16(__u16 x)
 {
 	__asm__("dep %0, 15, 8, %0\n\t"		/* deposit 00ab -> 0bab */
 		"shd %%r0, %0, 8, %0"		/* shift 000000ab -> 00ba */
@@ -15,7 +15,7 @@
 	return x;
 }
 
-static __inline__ __attribute_const__ __u32 ___arch__swab24(__u32 x)
+static __inline__ __u32 ___arch__swab24(__u32 x)
 {
 	__asm__("shd %0, %0, 8, %0\n\t"		/* shift xabcxabc -> cxab */
 		"dep %0, 15, 8, %0\n\t"		/* deposit cxab -> cbab */
@@ -25,7 +25,7 @@
 	return x;
 }
 
-static __inline__ __attribute_const__ __u32 ___arch__swab32(__u32 x)
+static __inline__ __u32 ___arch__swab32(__u32 x)
 {
 	unsigned int temp;
 	__asm__("shd %0, %0, 16, %1\n\t"	/* shift abcdabcd -> cdab */
@@ -48,7 +48,7 @@
 **      HSHR    67452301 -> *6*4*2*0 into %0
 **      OR      %0 | %1  -> 76543210 into %0 (all done!)
 */
-static __inline__ __attribute_const__ __u64 ___arch__swab64(__u64 x) {
+static __inline__ __u64 ___arch__swab64(__u64 x) {
 	__u64 temp;
 	__asm__("permh,3210 %0, %0\n\t"
 		"hshl %0, 8, %1\n\t"
@@ -61,7 +61,7 @@
 #define __arch__swab64(x) ___arch__swab64(x)
 #define __BYTEORDER_HAS_U64__
 #elif !defined(__STRICT_ANSI__)
-static __inline__ __attribute_const__ __u64 ___arch__swab64(__u64 x)
+static __inline__ __u64 ___arch__swab64(__u64 x)
 {
 	__u32 t1 = ___arch__swab32((__u32) x);
 	__u32 t2 = ___arch__swab32((__u32) (x >> 32));
Index: include/asm-sh/byteorder.h
===================================================================
--- include/asm-sh/byteorder.h.orig	2006-09-19 23:42:06.000000000 -0400
+++ include/asm-sh/byteorder.h	2006-10-07 09:02:17.370543699 -0400
@@ -8,7 +8,7 @@
 #include <asm/types.h>
 #include <linux/compiler.h>
 
-static __inline__ __attribute_const__ __u32 ___arch__swab32(__u32 x)
+static __inline__ __u32 ___arch__swab32(__u32 x)
 {
 	__asm__("swap.b	%0, %0\n\t"
 		"swap.w %0, %0\n\t"
@@ -18,7 +18,7 @@
 	return x;
 }
 
-static __inline__ __attribute_const__ __u16 ___arch__swab16(__u16 x)
+static __inline__ __u16 ___arch__swab16(__u16 x)
 {
 	__asm__("swap.b %0, %0"
 		: "=r" (x)
Index: include/asm-v850/byteorder.h
===================================================================
--- include/asm-v850/byteorder.h.orig	2006-09-19 23:42:06.000000000 -0400
+++ include/asm-v850/byteorder.h	2006-10-07 09:02:17.374544106 -0400
@@ -19,14 +19,14 @@
 
 #ifdef __GNUC__
 
-static __inline__ __attribute_const__ __u32 ___arch__swab32 (__u32 word)
+static __inline__ __u32 ___arch__swab32 (__u32 word)
 {
 	__u32 res;
 	__asm__ ("bsw %1, %0" : "=r" (res) : "r" (word));
 	return res;
 }
 
-static __inline__ __attribute_const__ __u16 ___arch__swab16 (__u16 half_word)
+static __inline__ __u16 ___arch__swab16 (__u16 half_word)
 {
 	__u16 res;
 	__asm__ ("bsh %1, %0" : "=r" (res) : "r" (half_word));
Index: include/asm-x86_64/byteorder.h
===================================================================
--- include/asm-x86_64/byteorder.h.orig	2006-09-19 23:42:06.000000000 -0400
+++ include/asm-x86_64/byteorder.h	2006-10-07 09:02:17.374544106 -0400
@@ -6,13 +6,13 @@
 
 #ifdef __GNUC__
 
-static __inline__ __attribute_const__ __u64 ___arch__swab64(__u64 x)
+static __inline__ __u64 ___arch__swab64(__u64 x)
 {
 	__asm__("bswapq %0" : "=r" (x) : "0" (x));
 	return x;
 }
 
-static __inline__ __attribute_const__ __u32 ___arch__swab32(__u32 x)
+static __inline__ __u32 ___arch__swab32(__u32 x)
 {
 	__asm__("bswapl %0" : "=r" (x) : "0" (x));
 	return x;
Index: include/linux/byteorder/swab.h
===================================================================
--- include/linux/byteorder/swab.h.orig	2006-10-07 09:02:17.062512303 -0400
+++ include/linux/byteorder/swab.h	2006-10-07 09:02:17.374544106 -0400
@@ -130,7 +130,7 @@
 #endif /* OPTIMIZE */
 
 
-static __inline__ __attribute_const__ __u16 __fswab16(__u16 x)
+static __inline__ __u16 __fswab16(__u16 x)
 {
 	return __arch__swab16(x);
 }
@@ -143,7 +143,7 @@
 	__arch__swab16s(addr);
 }
 
-static __inline__ __attribute_const__ __u32 __fswab32(__u32 x)
+static __inline__ __u32 __fswab32(__u32 x)
 {
 	return __arch__swab32(x);
 }
@@ -158,7 +158,7 @@
 
 #ifndef __STRICT_ANSI__
 #ifdef __BYTEORDER_HAS_U64__
-static __inline__ __attribute_const__ __u64 __fswab64(__u64 x)
+static __inline__ __u64 __fswab64(__u64 x)
 {
 #  ifdef __SWAB_64_THRU_32__
 	__u32 h = x >> 32;
Index: include/asm-cris/arch-v10/byteorder.h
===================================================================
--- include/asm-cris/arch-v10/byteorder.h.orig	2006-09-19 23:42:06.000000000 -0400
+++ include/asm-cris/arch-v10/byteorder.h	2006-10-07 09:02:17.378544514 -0400
@@ -9,14 +9,14 @@
  * them together into ntohl etc.
  */
 
-static inline __attribute_const__ __u32 ___arch__swab32(__u32 x)
+static inline __u32 ___arch__swab32(__u32 x)
 {
 	__asm__ ("swapwb %0" : "=r" (x) : "0" (x));
   
 	return(x);
 }
 
-static inline __attribute_const__ __u16 ___arch__swab16(__u16 x)
+static inline __u16 ___arch__swab16(__u16 x)
 {
 	__asm__ ("swapb %0" : "=r" (x) : "0" (x));
 	
Index: include/asm-powerpc/byteorder.h
===================================================================
--- include/asm-powerpc/byteorder.h.orig	2006-09-19 23:42:06.000000000 -0400
+++ include/asm-powerpc/byteorder.h	2006-10-07 09:02:17.378544514 -0400
@@ -40,7 +40,7 @@
 	__asm__ __volatile__ ("stwbrx %1,0,%2" : "=m" (*addr) : "r" (val), "r" (addr));
 }
 
-static __inline__ __attribute_const__ __u16 ___arch__swab16(__u16 value)
+static __inline__ __u16 ___arch__swab16(__u16 value)
 {
 	__u16 result;
 
@@ -50,7 +50,7 @@
 	return result;
 }
 
-static __inline__ __attribute_const__ __u32 ___arch__swab32(__u32 value)
+static __inline__ __u32 ___arch__swab32(__u32 value)
 {
 	__u32 result;
 
Index: include/asm-arm/byteorder.h
===================================================================
--- include/asm-arm/byteorder.h.orig	2006-10-07 09:02:59.286816053 -0400
+++ include/asm-arm/byteorder.h	2006-10-07 09:03:19.256851602 -0400
@@ -18,7 +18,7 @@
 #include <linux/compiler.h>
 #include <asm/types.h>
 
-static inline __attribute_const__ __u32 ___arch__swab32(__u32 x)
+static __inline__ __attribute_const__ __u32 ___arch__swab32(__u32 x)
 {
 	__u32 t;
 
@@ -29,7 +29,7 @@
 		 * right thing and not screw it up to different degrees
 		 * depending on the gcc version.
 		 */
-		asm ("eor\t%0, %1, %1, ror #16" : "=r" (t) : "r" (x));
+		__asm__ ("eor\t%0, %1, %1, ror #16" : "=r" (t) : "r" (x));
 	} else
 #endif
 		t = x ^ ((x << 16) | (x >> 16)); /* eor r1,r0,r0,ror #16 */

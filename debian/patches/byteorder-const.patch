Update 2005-05-27 gotom.
Disable asm-ppc64/byteorder.h patch, move it to asm-ppc64-swab64.h

--- include.orig/asm-alpha/byteorder.h	2005-04-21 09:03:16.000000000 +0900
+++ include/asm-alpha/byteorder.h	2005-05-27 21:39:42.537551683 +0900
@@ -7,7 +7,7 @@
 
 #ifdef __GNUC__
 
-static __inline __attribute_const__ __u32 __arch__swab32(__u32 x)
+static __inline __u32 __arch__swab32(__u32 x)
 {
 	/*
 	 * Unfortunately, we can't use the 6 instruction sequence
--- include.orig/asm-cris/arch-v10/byteorder.h	2005-04-21 09:03:16.000000000 +0900
+++ include/asm-cris/arch-v10/byteorder.h	2005-05-27 21:40:50.951268236 +0900
@@ -9,14 +9,14 @@
  * them together into ntohl etc.
  */
 
-extern __inline__ __attribute_const__ __u32 ___arch__swab32(__u32 x)
+extern __inline__ __u32 ___arch__swab32(__u32 x)
 {
 	__asm__ ("swapwb %0" : "=r" (x) : "0" (x));
   
 	return(x);
 }
 
-extern __inline__ __attribute_const__ __u16 ___arch__swab16(__u16 x)
+extern __inline__ __u16 ___arch__swab16(__u16 x)
 {
 	__asm__ ("swapb %0" : "=r" (x) : "0" (x));
 	
--- include.orig/asm-m68k/byteorder.h	2005-04-21 09:03:16.000000000 +0900
+++ include/asm-m68k/byteorder.h	2005-05-27 21:42:28.255064498 +0900
@@ -6,7 +6,7 @@
 
 #ifdef __GNUC__
 
-static __inline__ __attribute_const__ __u32 ___arch__swab32(__u32 val)
+static __inline__ __u32 ___arch__swab32(__u32 val)
 {
 	__asm__("rolw #8,%0; swap %0; rolw #8,%0" : "=d" (val) : "0" (val));
 	return val;
--- include.orig/asm-parisc/byteorder.h	2005-04-21 09:03:16.000000000 +0900
+++ include/asm-parisc/byteorder.h	2005-05-27 21:43:25.651276042 +0900
@@ -6,7 +6,7 @@
 
 #ifdef __GNUC__
 
-static __inline__ __attribute_const__ __u16 ___arch__swab16(__u16 x)
+static __inline__ __u16 ___arch__swab16(__u16 x)
 {
 	__asm__("dep %0, 15, 8, %0\n\t"		/* deposit 00ab -> 0bab */
 		"shd %%r0, %0, 8, %0"		/* shift 000000ab -> 00ba */
@@ -15,7 +15,7 @@
 	return x;
 }
 
-static __inline__ __attribute_const__ __u32 ___arch__swab24(__u32 x)
+static __inline__ __u32 ___arch__swab24(__u32 x)
 {
 	__asm__("shd %0, %0, 8, %0\n\t"		/* shift xabcxabc -> cxab */
 		"dep %0, 15, 8, %0\n\t"		/* deposit cxab -> cbab */
@@ -25,7 +25,7 @@
 	return x;
 }
 
-static __inline__ __attribute_const__ __u32 ___arch__swab32(__u32 x)
+static __inline__ __u32 ___arch__swab32(__u32 x)
 {
 	unsigned int temp;
 	__asm__("shd %0, %0, 16, %1\n\t"	/* shift abcdabcd -> cdab */
@@ -48,7 +48,7 @@
 **      HSHR    67452301 -> *6*4*2*0 into %0
 **      OR      %0 | %1  -> 76543210 into %0 (all done!)
 */
-static __inline__ __attribute_const__ __u64 ___arch__swab64(__u64 x) {
+static __inline__ __u64 ___arch__swab64(__u64 x) {
 	__u64 temp;
 	__asm__("permh,3210 %0, %0\n\t"
 		"hshl %0, 8, %1\n\t"
@@ -61,7 +61,7 @@
 #define __arch__swab64(x) ___arch__swab64(x)
 #define __BYTEORDER_HAS_U64__
 #elif !defined(__STRICT_ANSI__)
-static __inline__ __attribute_const__ __u64 ___arch__swab64(__u64 x)
+static __inline__ __u64 ___arch__swab64(__u64 x)
 {
 	__u32 t1 = ___arch__swab32((__u32) x);
 	__u32 t2 = ___arch__swab32((__u32) (x >> 32));
--- include.orig/asm-ppc/byteorder.h	2005-04-21 09:03:16.000000000 +0900
+++ include/asm-ppc/byteorder.h	2005-05-27 21:43:47.623294516 +0900
@@ -33,7 +33,7 @@
 	__asm__ __volatile__ ("stwbrx %1,0,%2" : "=m" (*addr) : "r" (val), "r" (addr));
 }
 
-static __inline__ __attribute_const__ __u16 ___arch__swab16(__u16 value)
+static __inline__ __u16 ___arch__swab16(__u16 value)
 {
 	__u16 result;
 
@@ -41,7 +41,7 @@
 	return result;
 }
 
-static __inline__ __attribute_const__ __u32 ___arch__swab32(__u32 value)
+static __inline__ __u32 ___arch__swab32(__u32 value)
 {
 	__u32 result;
 
#--- include.orig/asm-ppc64/byteorder.h	2005-04-21 09:03:16.000000000 +0900
#+++ include/asm-ppc64/byteorder.h	2005-05-27 21:44:08.589474205 +0900
#@@ -41,7 +41,7 @@
# }
# 
# #if 0
#-static __inline__ __attribute_const__ __u16 ___arch__swab16(__u16 value)
#+static __inline__ __u16 ___arch__swab16(__u16 value)
# {
# 	__u16 result;
# 
#@@ -51,7 +51,7 @@
# 	return result;
# }
# 
#-static __inline__ __attribute_const__ __u32 ___arch__swab32(__u32 value)
#+static __inline__ __u32 ___arch__swab32(__u32 value)
# {
# 	__u32 result;
# 
#@@ -63,7 +63,7 @@
# 	return result;
# }
# 
#-static __inline__ __attribute_const__ __u64 ___arch__swab64(__u64 value)
#+static __inline__ __u64 ___arch__swab64(__u64 value)
# {
# 	__u64 result;
# #error implement me
--- include.orig/asm-sh/byteorder.h	2005-04-21 09:03:16.000000000 +0900
+++ include/asm-sh/byteorder.h	2005-05-27 21:44:57.552894641 +0900
@@ -8,7 +8,7 @@
 #include <asm/types.h>
 #include <linux/compiler.h>
 
-static __inline__ __attribute_const__ __u32 ___arch__swab32(__u32 x)
+static __inline__ __u32 ___arch__swab32(__u32 x)
 {
 	__asm__("swap.b	%0, %0\n\t"
 		"swap.w %0, %0\n\t"
@@ -18,7 +18,7 @@
 	return x;
 }
 
-static __inline__ __attribute_const__ __u16 ___arch__swab16(__u16 x)
+static __inline__ __u16 ___arch__swab16(__u16 x)
 {
 	__asm__("swap.b %0, %0"
 		: "=r" (x)
--- include.orig/asm-sh64/byteorder.h	2005-04-21 09:03:16.000000000 +0900
+++ include/asm-sh64/byteorder.h	2005-05-27 21:50:48.279743143 +0900
@@ -14,7 +14,7 @@
 
 #include <asm/types.h>
 
-static __inline__ __const__ __u32 ___arch__swab32(__u32 x)
+static __inline__ __u32 ___arch__swab32(__u32 x)
 {
 	__asm__("byterev	%0, %0\n\t"
 		"shari		%0, 32, %0"
@@ -23,7 +23,7 @@
 	return x;
 }
 
-static __inline__ __const__ __u16 ___arch__swab16(__u16 x)
+static __inline__ __u16 ___arch__swab16(__u16 x)
 {
 	__asm__("byterev	%0, %0\n\t"
 		"shari		%0, 48, %0"
--- include.orig/asm-v850/byteorder.h	2005-04-21 09:03:16.000000000 +0900
+++ include/asm-v850/byteorder.h	2005-05-27 21:45:51.315669274 +0900
@@ -19,14 +19,14 @@
 
 #ifdef __GNUC__
 
-static __inline__ __attribute_const__ __u32 ___arch__swab32 (__u32 word)
+static __inline__ __u32 ___arch__swab32 (__u32 word)
 {
 	__u32 res;
 	__asm__ ("bsw %1, %0" : "=r" (res) : "r" (word));
 	return res;
 }
 
-static __inline__ __attribute_const__ __u16 ___arch__swab16 (__u16 half_word)
+static __inline__ __u16 ___arch__swab16 (__u16 half_word)
 {
 	__u16 res;
 	__asm__ ("bsh %1, %0" : "=r" (res) : "r" (half_word));
--- include.orig/asm-x86_64/byteorder.h	2005-04-21 09:03:16.000000000 +0900
+++ include/asm-x86_64/byteorder.h	2005-05-27 22:06:28.905489053 +0900
@@ -6,13 +6,13 @@
 
 #ifdef __GNUC__
 
-static __inline__ __attribute_const__ __u64 ___arch__swab64(__u64 x)
+static __inline__ __u64 ___arch__swab64(__u64 x)
 {
 	__asm__("bswapq %0" : "=r" (x) : "0" (x));
 	return x;
 }
 
-static __inline__ __attribute_const__ __u32 ___arch__swab32(__u32 x)
+static __inline__ __u32 ___arch__swab32(__u32 x)
 {
 	__asm__("bswapl %0" : "=r" (x) : "0" (x));
 	return x;
--- include.orig/linux/byteorder/swab.h	2005-05-27 22:05:04.347036459 +0900
+++ include/linux/byteorder/swab.h	2005-05-27 22:06:52.423277697 +0900
@@ -130,7 +130,7 @@
 #endif /* OPTIMIZE */
 
 
-static __inline__ __attribute_const__ __u16 __fswab16(__u16 x)
+static __inline__ __u16 __fswab16(__u16 x)
 {
 	return __arch__swab16(x);
 }
@@ -143,7 +143,7 @@
 	__arch__swab16s(addr);
 }
 
-static __inline__ __attribute_const__ __u32 __fswab32(__u32 x)
+static __inline__ __u32 __fswab32(__u32 x)
 {
 	return __arch__swab32(x);
 }
@@ -158,7 +158,7 @@
 
 #ifndef __STRICT_ANSI__
 #ifdef __BYTEORDER_HAS_U64__
-static __inline__ __attribute_const__ __u64 __fswab64(__u64 x)
+static __inline__ __u64 __fswab64(__u64 x)
 {
 #  ifdef __SWAB_64_THRU_32__
 	__u32 h = x >> 32;
--- include.orig/linux/byteorder/swabb.h	2005-04-21 09:03:16.000000000 +0900
+++ include/linux/byteorder/swabb.h	2005-05-27 22:07:19.637561723 +0900
@@ -92,7 +92,7 @@
 #endif /* OPTIMIZE */
 
 
-static __inline__ __const__ __u32 __fswahw32(__u32 x)
+static __inline__ __u32 __fswahw32(__u32 x)
 {
 	return __arch__swahw32(x);
 }
@@ -106,7 +106,7 @@
 }
 
 
-static __inline__ __const__ __u32 __fswahb32(__u32 x)
+static __inline__ __u32 __fswahb32(__u32 x)
 {
 	return __arch__swahb32(x);
 }
